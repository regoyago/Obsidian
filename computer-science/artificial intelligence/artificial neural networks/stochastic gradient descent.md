the stochastic gradient descent is an iterative method for optimising an objective function with suitable [[smoothness]] properties
it can be regarded as a [[stochastic approximation]] of [[gradient descent]] optimisation, since it replaces the actual gradient by an estimate thereof

specially in high-dimensional optimisation problems this reduces the very high computational burden, archiving faster iterations in trade for lower convergence rate

#artificial_neural_networks 