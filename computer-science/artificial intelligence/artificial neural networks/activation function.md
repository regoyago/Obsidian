in [[artificial neural network]]s, the activation function of a node is a [[mathematical function]] that defines the output of that node given an input or set of inputs

a standard [[integrated system]] can be seen as a [[digital network]] of activation functions that can be `on` or  `of` depending on the input
this is similar to the [[linear perceptron]] in [[artificial neural network]]s
however, only nonlinear[^1] activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities 

the most common activation functions can be divided in three categories:
- [[ridge function]]s
	- [[linear equation]]
	- [[ReLU]]
	- [[heaviside]]
	- [[logistic]]
- [[radial function]]s
	- [[gaussian]]
	- [[multiquadratic]]s
	- [[polyharmonic spline]]s
- [[fold function]]s

[^1]: this is caused by the fact that on a bigger system ( [[artificial neural network]]s)  the concatenation of the output of several [[artificial neuron]]s  ([[linear regression]]s essentially) would be merged into one, since the effect of applying several linear operations is equivalent to make just one

#artificial_neural_networks 